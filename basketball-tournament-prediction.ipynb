{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91497,"databundleVersionId":11320667,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T00:44:27.579814Z","iopub.execute_input":"2025-03-11T00:44:27.580090Z","iopub.status.idle":"2025-03-11T00:44:27.602720Z","shell.execute_reply.started":"2025-03-11T00:44:27.580069Z","shell.execute_reply":"2025-03-11T00:44:27.601984Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/march-machine-learning-mania-2025/Conferences.csv\n/kaggle/input/march-machine-learning-mania-2025/SeedBenchmarkStage1.csv\n/kaggle/input/march-machine-learning-mania-2025/WNCAATourneyDetailedResults.csv\n/kaggle/input/march-machine-learning-mania-2025/WRegularSeasonCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/MNCAATourneySeedRoundSlots.csv\n/kaggle/input/march-machine-learning-mania-2025/MRegularSeasonDetailedResults.csv\n/kaggle/input/march-machine-learning-mania-2025/MNCAATourneyCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/MGameCities.csv\n/kaggle/input/march-machine-learning-mania-2025/WSecondaryTourneyCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/WGameCities.csv\n/kaggle/input/march-machine-learning-mania-2025/MSeasons.csv\n/kaggle/input/march-machine-learning-mania-2025/WNCAATourneySlots.csv\n/kaggle/input/march-machine-learning-mania-2025/MSecondaryTourneyTeams.csv\n/kaggle/input/march-machine-learning-mania-2025/SampleSubmissionStage2.csv\n/kaggle/input/march-machine-learning-mania-2025/Cities.csv\n/kaggle/input/march-machine-learning-mania-2025/MTeamSpellings.csv\n/kaggle/input/march-machine-learning-mania-2025/MRegularSeasonCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/MMasseyOrdinals.csv\n/kaggle/input/march-machine-learning-mania-2025/MSecondaryTourneyCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/WTeams.csv\n/kaggle/input/march-machine-learning-mania-2025/WConferenceTourneyGames.csv\n/kaggle/input/march-machine-learning-mania-2025/MNCAATourneySlots.csv\n/kaggle/input/march-machine-learning-mania-2025/MNCAATourneySeeds.csv\n/kaggle/input/march-machine-learning-mania-2025/WNCAATourneyCompactResults.csv\n/kaggle/input/march-machine-learning-mania-2025/WSeasons.csv\n/kaggle/input/march-machine-learning-mania-2025/WNCAATourneySeeds.csv\n/kaggle/input/march-machine-learning-mania-2025/MTeamCoaches.csv\n/kaggle/input/march-machine-learning-mania-2025/MConferenceTourneyGames.csv\n/kaggle/input/march-machine-learning-mania-2025/WRegularSeasonDetailedResults.csv\n/kaggle/input/march-machine-learning-mania-2025/MNCAATourneyDetailedResults.csv\n/kaggle/input/march-machine-learning-mania-2025/WTeamSpellings.csv\n/kaggle/input/march-machine-learning-mania-2025/MTeamConferences.csv\n/kaggle/input/march-machine-learning-mania-2025/MTeams.csv\n/kaggle/input/march-machine-learning-mania-2025/WTeamConferences.csv\n/kaggle/input/march-machine-learning-mania-2025/SampleSubmissionStage1.csv\n/kaggle/input/march-machine-learning-mania-2025/WSecondaryTourneyTeams.csv\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n#-------------------------------------------------------------------\n# 1) LOAD THE DATA\n#-------------------------------------------------------------------\n# Adjust file paths as needed for your environment:\nMTeams = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/MTeams.csv\")\nWTeams = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/WTeams.csv\")\nMSeasons = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/MSeasons.csv\")\nWSeasons = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/WSeasons.csv\")\n\n# Regular-season (compact) results – men’s\nMRegSeasonCompact = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/MRegularSeasonCompactResults.csv\")\n\n# NCAA Tournament (compact) results – men’s\nMNCAATourneyCompact = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/MNCAATourneyCompactResults.csv\")\n\n# Seeds – men’s\nMNCAATourneySeeds = pd.read_csv(\"/kaggle/input/march-machine-learning-mania-2025/MNCAATourneySeeds.csv\")\n\n#-------------------------------------------------------------------\n# 2) QUICK LOOK AT THE DATA\n#-------------------------------------------------------------------\nprint(\"MTeams sample:\")\nprint(MTeams.head(), \"\\n\")\n\nprint(\"MSeasons sample:\")\nprint(MSeasons.head(), \"\\n\")\n\nprint(\"MRegularSeasonCompactResults sample:\")\nprint(MRegSeasonCompact.head(), \"\\n\")\n\n#-------------------------------------------------------------------\n# 3) SIMPLE EDA / FEATURE EXAMPLE\n#    Illustrate how to create a small dataset for a quick model\n#-------------------------------------------------------------------\n# We’ll demonstrate a small pipeline using men’s regular-season results.\n# Basic idea: treat each game as one observation for a binary classification.\n\n# Create a label: 1 if Home/Winning Team was 'WTeamID', 0 otherwise\n# But we actually want to represent the probability that \"Team A beats Team B.\"\n# For now, let's just do something very simplified, ignoring neutral sites, etc.\n\n# We'll rename columns to keep them standardized:\ndf = MRegSeasonCompact.copy()\ndf['TeamA'] = df['WTeamID']\ndf['TeamB'] = df['LTeamID']\ndf['TeamA_Score'] = df['WScore']\ndf['TeamB_Score'] = df['LScore']\n\n# The \"target\": TeamA won => 1, TeamA lost => 0\ndf['TeamA_Won'] = 1\n\n# But we only have the scenario \"TeamA is WTeamID.\" If you want\n# both perspectives (TeamA vs. TeamB and TeamB vs. TeamA), you could\n# re-stack the dataset or do other transformations. For simplicity,\n# we keep it single-direction here.\n\n# Simple feature: point differential\ndf['ScoreDiff'] = df['TeamA_Score'] - df['TeamB_Score']\n\n# Let's define some minimal features:\nfeatures = ['ScoreDiff']  # obviously very naive\nX = df[features]\ny = df['TeamA_Won']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    random_state=42)\n\n# Fit a trivial logistic regression (again, very naive)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ntrain_acc = model.score(X_train, y_train)\ntest_acc = model.score(X_test, y_test)\nprint(f\"Train Accuracy: {train_acc:.3f}\")\nprint(f\"Test  Accuracy: {test_acc:.3f}\")\n\n#-------------------------------------------------------------------\n# 4) CREATE A SAMPLE SUBMISSION\n#-------------------------------------------------------------------\n# In 2025, you have to predict *every* possible matchup for men’s and women’s teams.\n# For demonstration, let’s show how you’d generate matchups just for men’s TeamIDs.\n# (In reality, you’d combine the men’s and women’s IDs and do them all.)\n\nall_mens_teams = MTeams['TeamID'].unique()  # e.g., 1000-1999\n\n# Sort them so we can do combinations where TeamA < TeamB\nall_mens_teams = sorted(all_mens_teams)\n\nmatchup_rows = []\nfor A, B in itertools.combinations(all_mens_teams, 2):\n    # -----------------------------------------------------\n    # This is where you'd compute the probability that team A\n    # would beat team B. For instance:\n    #   p = my_model.predict_proba( feature_vector_for(A,B) )\n    #\n    # But we have not engineered a robust feature pipeline for A vs. B.\n    # So we'll just put a placeholder probability (like 0.5).\n    # -----------------------------------------------------\n    p = 0.50\n    row_id = f\"2025_{A}_{B}\"\n    matchup_rows.append((row_id, p))\n\n# Convert to a DataFrame\nsubmission_df = pd.DataFrame(matchup_rows, columns=[\"ID\", \"Pred\"])\n\n# Quick preview\nprint(submission_df.head(10))\n\n# Save to CSV\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nprint(\"\\nSample submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T00:44:27.603662Z","iopub.execute_input":"2025-03-11T00:44:27.603952Z","iopub.status.idle":"2025-03-11T00:44:27.875055Z","shell.execute_reply.started":"2025-03-11T00:44:27.603917Z","shell.execute_reply":"2025-03-11T00:44:27.873939Z"}},"outputs":[{"name":"stdout","text":"MTeams sample:\n   TeamID     TeamName  FirstD1Season  LastD1Season\n0    1101  Abilene Chr           2014          2025\n1    1102    Air Force           1985          2025\n2    1103        Akron           1985          2025\n3    1104      Alabama           1985          2025\n4    1105  Alabama A&M           2000          2025 \n\nMSeasons sample:\n   Season     DayZero RegionW    RegionX    RegionY    RegionZ\n0    1985  10/29/1984    East       West    Midwest  Southeast\n1    1986  10/28/1985    East    Midwest  Southeast       West\n2    1987  10/27/1986    East  Southeast    Midwest       West\n3    1988  11/02/1987    East    Midwest  Southeast       West\n4    1989  10/31/1988    East       West    Midwest  Southeast \n\nMRegularSeasonCompactResults sample:\n   Season  DayNum  WTeamID  WScore  LTeamID  LScore WLoc  NumOT\n0    1985      20     1228      81     1328      64    N      0\n1    1985      25     1106      77     1354      70    H      0\n2    1985      25     1112      63     1223      56    H      0\n3    1985      25     1165      70     1432      54    H      0\n4    1985      25     1192      86     1447      74    H      0 \n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-d028a176e27d>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Fit a trivial logistic regression (again, very naive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0;34m\"This solver needs samples of at least 2 classes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1"],"ename":"ValueError","evalue":"This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1","output_type":"error"}],"execution_count":6}]}